# Exercise Tracker

**Exercise Tracker** is a Python-based system that uses computer vision to track and evaluate exercise form in real time.  
It can count repetitions (e.g., squats), analyze movement quality, and provide feedback using heuristics and trained ML models.  

The project focuses on the **squat exercise**. Using the [MediaPipe](https://developers.google.com/mediapipe) library, pose landmarks are extracted frame by frame and stored in CSV format. These raw data are then processed and used to train a custom classification model.  

The workflow was:  
1. **Feature Engineering:** Researched which metrics define a "proper" squat (e.g., angles, alignment) and designed functions to compute them.  
2. **Data Collection:** Captured pose data of people performing squats.  
3. **Model Development:** Trained a model on the collected features to distinguish between correct and incorrect squat form.  
4. **Prediction:** Integrated the model into a real-time application that can evaluate each repetition either via a live webcam feed or from prerecorded videos.  

In practice, the application continuously takes pose data during each squat repetition, evaluates the movement, and labels it as **correct** or **incorrect**.  

<img width="572" height="635" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2025-08-19 160639" src="https://github.com/user-attachments/assets/b1550b71-fa87-465c-b681-06208be1f314" />
<img width="314" height="985" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2025-08-19 160837" src="https://github.com/user-attachments/assets/d6919f72-2ddc-4173-ba0d-502178d16cb1" />


## Features

- ğŸ“¹ **Real-time Tracking** â€“ Uses webcam or video files to capture and analyze squat movements.  
- ğŸ§ **Pose Estimation** â€“ Extracts body landmarks with MediaPipe and processes them into meaningful features (angles, alignments, etc.).  
- ğŸ“Š **Data Logging** â€“ Saves raw pose data into CSV files for further analysis or model training.  
- ğŸ¤– **Model-based Evaluation** â€“ Trained ML classifier to decide if each squat rep is correct or incorrect.  
- ğŸ”„ **Rep Counting** â€“ Automatically counts squat repetitions using a state machine (up/down detection).  
- ğŸ¥ **Video Support** â€“ If no camera is available, users can run predictions on sample videos located in `data/raw/`.


## Installation

### Requirements
- Python 3.10 or newer  
- A working webcam (optional, not required if using sample videos)  

### Setup

Clone the repository and install dependencies:

```bash
git clone https://github.com/emirhaan11/exercise-tracker.git
cd exercise-tracker
pip install -r requirements.txt
```

## Usage

You can run the application either with a **webcam** or with **video files**.  
If you donâ€™t have a camera, use the sample clips under `data/raw/`.

### 1) Webcam Inference

```bash
# Start real-time prediction with the default camera (index 0)
python -m main/pipeline_realtime_predict.py

# Predict on a sample video shipped in the repo
python -m main.video_predict --input data/raw/sample_squat.mp4
```

## Project Structure

```bash
exercise-tracker/
â”œâ”€ core/ # Pose estimator, feature extractors, counters, rules
â”œâ”€ data/
â”‚ â”œâ”€ raw/ # Sample input videos (use these if you have no camera)
â”‚ â””â”€ processed/ # Preprocessed datasets
â”œâ”€ docs/ # Notes
â”‚ â””â”€ README.md
â”œâ”€ main/
â”‚ â””â”€ pipeline.py # Pose analyze without ML model
â”‚ â”œâ”€ pipeline_realtime_predict.py # Entry point for webcam inference
â”‚ â””â”€ video_predict.py # Entry point for file-based inference
â”œâ”€ models/
â”‚ â””â”€ squat_clf.pkl # Trained classifier 
â”œâ”€ notebooks/ # Training notebooks
â”œâ”€ test/ # Unit tests
â”œâ”€ requirements.txt # Python dependencies

```
